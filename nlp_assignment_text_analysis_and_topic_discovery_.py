# -*- coding: utf-8 -*-
"""NLP Assignment: Text Analysis and Topic Discovery .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mN9UNN4T85-x2cqq9NIzCrSjyAl0mPTj
"""

#Tasks-1
# Text Preprocessing: Tokenize the text into words.
# libraries for tokenization
import nltk
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
# This is the dataset
docs = [
    "India is highly prone to earthquakes due to its position on the boundary of the Indian and Eurasian tectonic plates.",
    "The Himalayan region is the most earthquake-sensitive zone in the country.",
    "According to the seismic zoning map, India is divided into four major seismic zones: II, III, IV, and V.",
    "Zone V includes areas of the Himalayas and the North-East, which are most vulnerable to destructive earthquakes.",
    "Major cities like Delhi, Srinagar, and Guwahati fall under high seismic risk zones.",
    "The devastating 2001 Bhuj earthquake in Gujarat killed more than 20,000 people.",
    "Another severe earthquake struck Kashmir in 2005, affecting both India and Pakistan.",
    "The 2015 Nepal earthquake also caused significant tremors and damage in northern India.",
    "Earthquakes in India often result in loss of lives, property damage, and disruption of infrastructure.",
    "Poor building structures in rural and urban areas increase vulnerability to earthquake damage.",
    "The Indian government has launched programs to promote earthquake-resistant construction practices.",
    "The National Disaster Management Authority (NDMA) issues guidelines for earthquake preparedness.",
    "Early warning systems and community awareness campaigns help reduce risks.",
    "Scientists continuously study seismic activity in India to better predict future earthquakes.",
    "Preparedness, strict building codes, and disaster response planning are essential to minimize earthquake impact in India.",
    "Amazon delivers products very quickly, often before the expected date.",
    "The packaging quality is excellent, and items usually arrive in perfect condition.",
    "Customer service is very supportive and responds promptly to issues.",
    "Sometimes the product quality does not match the description, which is disappointing.",
    "Overall, Amazon is reliable for online shopping with a wide range of products.",
    "Flipkart often provides good discounts and deals during festive sales.",
    "The delivery is fast in metro cities but can be delayed in rural areas.",
    "Product quality is generally good, but returns and replacements take some time.",
    "The mobile app is user-friendly and makes shopping easy.",
    "Flipkart is a great option for buying electronics and fashion at reasonable prices.",
    "Seiko watches are very reliable, and the Grand Seiko line feels like true luxury at an affordable price.",
    "Citizen Eco-Drive is amazing because the watch runs on light and never needs a battery change.",
    "Casio G-Shock is super tough, stylish, and perfect for outdoor activities.",
    "Orient makes affordable automatic watches that look elegant and work smoothly.",
    "Minase has unique handcrafted designs that feel premium and showcase Japanese craftsmanship."
]
# Stopwords is a words don’t carry much meaning in text analysis
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def tokenizing(txt):
    txt = txt.lower() # Convert all text to lowercase
    txt = re.sub(r'[^a-z\s]', '', txt)  # Remove punctuation, numbers, and special characters, keeping only alphabets and spaces.
    tokens = word_tokenize(txt) # Split text into words
    tokens = [word for word in tokens if word not in stop_words] # Remove stopwords
    return tokens # Returns the cleaned list of words.

docs = [tokenizing(doc) for doc in docs] # applying tokenization to the each word in the dataset
print("Tokenize the text into words\n:", docs) # Print all tokenized sentences in one line

for i, sentence in enumerate(docs, 1):
    print(f"{sentence}") # Print each tokenized sentence on a new line

#Tasks-2
# TF-IDF Analysis
#Compute the TF-IDF scores for all words in the corpus.
#Identify The top 10 words with the highest TF-IDF scores in each document.

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english') # vectorizer object and tell it to remove English stopwords.
X = vectorizer.fit_transform([" ".join(doc) for doc in docs]) # join the sentence and TF-IDF scores for all documents.
feature_names = vectorizer.get_feature_names_out() # the list of all unique words

# Top 10 words per document
for i in range(len(docs)): # Loop through each document in the dataset
    tfidf_scores = zip(feature_names, X[i].toarray()[0]) # gives the TF-IDF scores and pair with words
    sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:10] # descending order of TF-IDF of top 10 words
    print(f"Doc {i+1}: {sorted_scores}") # prints the top 10 words

#Tasks-3
#Word2Vec Embeddings:
#Train a Word2Vec model on the corpus.
#Find the 5 most similar words for the words “data,” “analysis,” or any relevant word in your dataset.
#Visualize word embeddings using dimensionality reduction in PCA

w2v_model = Word2Vec(sentences=docs, vector_size=100, window=5, min_count=1, workers=4) # checking the words that appear at least once

# Similar words
print(w2v_model.wv.most_similar("earthquakes", topn=5)) # # similar words that gives the top 5 words most related to earthquakes.
print(w2v_model.wv.most_similar("amazon", topn=5)) # similar words that gives the top 5 words most related to amazon.

# Visualization (PCA)
words = list(w2v_model.wv.index_to_key) # training the Word2Vec model
X = w2v_model.wv[words] # Extract the vector or word embedding

# ploating the word embeddings using dimensionality reduction (PCA)
pca = PCA(n_components=2)
result = pca.fit_transform(X) # apply PCA on word vectors.

# Create a scatter plot of the reduced vectors.
plt.figure(figsize=(10,8))
plt.scatter(result[:,0], result[:,1])
# ploating the scatter plot of the first 50 words
for i, word in enumerate(words[:50]):  # first 50 words
    plt.annotate(word, xy=(result[i,0], result[i,1]))
plt.show()

#Tasks-4
#Topic Modeling:
#Apply Latent Dirichlet Allocation (LDA) to discover 3–5 topics in the corpus.
#List the top 5 words for each topic.
#Assign the most relevant topic to each document.
from gensim import corpora, models
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
# Text Preprocessing or tokenization
txt = [[word for word in doc.lower().split() if word not in stop_words] for doc in text_doc]
# Create a dictionary each unique word to a unique ID.
dictionary = corpora.Dictionary(docs)
# Convert each document into a Bag of Words
corpus = [dictionary.doc2bow(text) for text in docs]
# to discover 4 topics in the corpus.
lda_model = models.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=10)
# List the top 5 words for each topic.
print("\nLDA Topics:")
for idx, topic in lda_model.print_topics(num_words=5):
    print(f"Topic {idx+1}: {topic}")

# assigning the topic to each document
for i, row in enumerate(lda_model[corpus]):
    row = sorted(row, key=lambda x: x[1], reverse=True)
    print(f"Document {i+1} -> Topic {row[0][0]+1}")

import pyLDAvis.gensim_models as gensimvis
import pyLDAvis # library for visualizing LDA topic models
# Enable visualization
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary) # LDA visualization.
pyLDAvis.display(vis) # Display the LDA visualization.